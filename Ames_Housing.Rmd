---
title: "Ames Housing Price Predictor"
author: "Jake Hogan"
date: "12/4/2020"
output: 
  pdf_document:
    fig_caption: yes
header-includes:
  \usepackage{float}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.pos = "H", fig.align = "center")
```

# Executive Summary

The Ames Housing dataset contains 79 variables that describe almost every feature of residential homes in Ames, Iowa.  The variables describe anything from the number of bedrooms and bathrooms to whether or not the road leading up to the house is paved. The dataset is presented as part of a Kaggle competition [(kaggle.com)](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) in which the goal is to use the data set to predict the sale price of Ames homes as accurately as possible. Exploratory data analysis and visualization was used to clean the data and explore which variables would be the most valuable for predictions. Then random forests, gradient boosting, and a regularized generalized linear model were used in an ensemble to make final sale price predictions. The metric used to judge the sale price predictions is the RMSE of the log transformed sale prices. The final log RMSE of the predictions is 0.13972.

# Data Analysis

The Ames Housing dataset can be downloaded from the Kaggle website linked above but requires an account. I've uploaded the raw data zip file to my GitHub account for easy access [(github.com/hoganjr)]("https://github.com/hoganjr/Ames_Housing/raw/main/house-prices-advanced-regression-techniques.zip"). The dataset comes split into a train file and test file.  The train and test files contain 1460 and 1459 home sales, respectively. The test file has all sale prices removed.  The zip file also contains a description for each of the 79 variables in the dataset.

```{r download and organize data, echo = FALSE, message = FALSE, warning = FALSE}
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(kableExtra)) install.packages("kableExtra", repos = "http://cran.us.r-project.org")
if(!require(readxl)) install.packages("readxl", repos = "http://cran.us.r-project.org")
if(!require(ggcorrplot)) install.packages("ggcorrplot", repos = "http://cran.us.r-project.org")
if(!require(corrplot)) install.packages("corrplot", repos = "http://cran.us.r-project.org")
if(!require(plyr)) install.packages("plyr", repos = "http://cran.us.r-project.org")
if(!require(broom)) install.packages("broom", repos = "http://cran.us.r-project.org")
if(!require(dplyr)) install.packages("dplyr", repos = "http://cran.us.r-project.org")
if(!require(Rborist)) install.packages("Rborist", repos = "http://cran.us.r-project.org")
if(!require(xgboost)) install.packages("xgboost", repos = "http://cran.us.r-project.org")
if(!require(glmnet)) install.packages("glmnet", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)
library(data.table)

##########################################################
# Create data sets, validation set (final hold-out test set)
##########################################################

# Note: this process could take a couple of minutes

#download the zip file from my github repo and store in "AmesData" local dir
dl <- tempfile()
download.file(
  "https://github.com/hoganjr/Ames_Housing/raw/main/house-prices-advanced-regression-techniques.zip",dl)
unzip(dl, exdir = "AmesData")

#The data is delivered in 2 csv files: a train file and a test file. Reading those in.
train_raw <- read.csv("AmesData/train.csv")
test_raw <- read.csv("AmesData/test.csv")
```

Looking into the train data set shows that it has sale prices from 34,900 to 755,000 USD. The median sale price is 163,000 USD. Below is a distribution of sale prices. The distribution is right-skewed.

```{r sale price distribution, echo = FALSE, message = FALSE, fig.pos='H', fig.cap= "Distribution of Home Sale Prices"}
qplot(SalePrice, data = train_raw, bins = 30,
      color = I("black"),  
      main = "Sale Price Distribution", 
      xlab = "Sale Price (USD)", 
      ylab = "Home Count"
      ) #right-skewed
```

A log transformation of the sale prices creates a more normal distribution.

```{r log sale price distribution, echo = FALSE, message = FALSE, fig.pos='H', fig.cap=   "Distribution of Log Transformed Home Sale Prices"}
qplot(log(SalePrice), data = train_raw, bins = 30,
      color = I("black"),  
      main = "Log Transformed Sale Price Distribution", 
      xlab = "Log Sale Price (USD)", 
      ylab = "Home Count"
) #normalized
```

From intuition and personal experience I know that location (neighborhood), square footage, number of bedrooms and bathrooms, as well as age can all impact the sale price of a house. Here's the mean sale price by neighborhood.

```{r sale price by neighborhood, echo = FALSE, message = FALSE, fig.pos='H', fig.cap= "Mean Sale Price by Neighborhood"}
train_raw %>% group_by(Neighborhood) %>% 
  dplyr::summarize(n = n(), avg = mean(SalePrice), 
            se = sd(SalePrice)/sqrt(n()), 
            med = median(SalePrice)) %>%
  mutate(Neighborhood = reorder(Neighborhood, avg)) %>%
  ggplot(aes(x = Neighborhood, y = avg, ymin = avg - 2*se, ymax = avg + 2*se)) + 
  geom_point() +
  geom_errorbar() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  ggtitle("Average Sale Price by Neighborhood") +
  xlab("Neighborhood") +
  ylab("Average Sale Price (USD)") 
```

Clearly some neighborhoods are more valuable than others. The square footage data is provided in multiple variables which makes it less clear whether or not square footage matters for sale price.  However if we combine them to get the total square footage a trend emerges.

```{r sale price by square footage, echo = FALSE, message = FALSE, fig.pos='H', fig.cap= "Sale Price by Square Footage"}
#mean sale price by square footage. According to the documentation the total 
#square footage is GrLivArea (sq ft above ground) + TotalBsmtSF (total basement 
#square feet)
train_raw <- train_raw %>% mutate(TotalSF = GrLivArea + TotalBsmtSF) 
train_raw %>%
  ggplot(aes(TotalSF, SalePrice)) + geom_point() + 
  ggtitle("Sale Price vs. Total Square Footage") + xlab("Total Square Footage") +
  ylab("Sale Price (USD)") #seems to be 2 outliers beyond 7500 sq. ft.
```

```{r find the sf outliers, echo = FALSE, message = FALSE, warning = FALSE}
#find these two outliers and mark them for removal
outliers_sf <- which(train_raw$TotalSF > 7500)
```

There appears to be 2 outliers with square footages over 7,500 but with low sale prices. As for bathrooms, the dataset again splits these up into four categories (Basement Full Baths, Basement Half Baths, Above Ground Full Baths and Above Ground Half Baths).  The four plots below show how quantities of each relate to sale price.

```{r sale price by bathroom type, echo = FALSE, message = FALSE, fig.pos='H', fig.cap= "Sale Price by Bathroom Type"}
plot1 <- train_raw %>% ggplot(aes(BsmtFullBath, SalePrice)) + geom_point() +
  xlab("Number of Basement Full Bathrooms") +
  ylab("Sale Price (USD)")
plot2 <- train_raw %>% ggplot(aes(BsmtHalfBath, SalePrice)) + geom_point() +
  xlab("Number of Basement Half Bathrooms") +
  ylab("Sale Price (USD)")
plot3 <- train_raw %>% ggplot(aes(FullBath, SalePrice)) + geom_point() +
  xlab("Number of Above Ground Full Bathrooms") +
  ylab("Sale Price (USD)")
plot4 <- train_raw %>% ggplot(aes(HalfBath, SalePrice)) + geom_point() +
  xlab("Number of Above Ground Half Bathrooms") +
  ylab("Sale Price (USD)")

gridExtra::grid.arrange(plot1, plot2, plot3, plot4, ncol = 2)
```

It appears above ground full baths may have a relationship with sale price but the rest are unclear. Let's combine them all to create a total bathrooms variable. 

```{r sale price by bathroom total, echo = FALSE, message = FALSE, fig.pos='H', fig.cap= "Sale Price by Total Bathrooms"}
#Creating total baths variable.
train_raw <- train_raw %>% mutate(TotalBaths = BsmtFullBath + 
                                    0.5*BsmtHalfBath + FullBath + 0.5*HalfBath) 
train_raw %>% ggplot(aes(TotalBaths, SalePrice)) + 
  geom_point() +
  ggtitle("Total Bathrooms vs. Sale Price") + xlab("Number of Bathrooms") +
  ylab("Sale Price (USD)")
```

```{r find the baths outliers, echo = FALSE, message = FALSE, warning = FALSE}
#find these two outliers and mark them for removal
outliers_baths <- which(train_raw$TotalBaths > 4.5)
```

There is a clearer trend now with a couple of outliers above 4.5 total baths. Now let's look at bedrooms. The only data available is bedrooms above ground. There doesn't appear to be a strong relationship with sale price.

```{r sale price by bed total, echo = FALSE, message = FALSE, fig.pos='H', fig.cap= "Sale Price by Total Bedrooms"}
train_raw %>% ggplot(aes(BedroomAbvGr, SalePrice)) +
  geom_point() +
  ggtitle("Bedrooms Above Grade vs. Sale Price") + xlab("Number of Bedrooms Above Grade") +
  ylab("Sale Price (USD)") 
#doesn't appear to be an obvious relationship here.
```

There appears to be two variables relating to the age of the house, the year it was built and the year it was remodeled.  If it hasn't been remodeled then the year it was remodeled matches the year it was built. Here are both variables plotted against sale price.

```{r sale price by year built and remodeled, echo = FALSE, message = FALSE, fig.pos='H', fig.cap= "Sale Price vs. Year Built and Year Remodeled"}
train_raw %>%
  ggplot(aes(x = value, SalePrice, color = Parameter)) + 
  geom_point(aes(x = YearBuilt, col = "Year Built")) +
  geom_point(aes(x = YearRemodAdd, col = "Year Remodeled")) +
  ggtitle("Sale Price vs. Year Built or Remodeled") + xlab("Year Built/Remodeled") +
  ylab("Sale Price (USD)") + theme(legend.position = "right")
```

There's no obvious distinction between the two parameters but there appears to be something odd with the year remodeled data. It appears that no remodeling was done prior to 1950 and that a spike in renovations occurred in the early 1950s.

```{r distribution of years remodeled, echo = FALSE, message = FALSE, fig.pos='H', fig.cap= "Distribution of Year Remodeled"}
qplot(YearRemodAdd, data = train_raw, bins = 10, color = I("black"),  
      main = "Year Remodeled Distribution", 
      xlab = "Year Remodeled", 
      ylab = "Remodel Year Count",
      ) 
```

It's possible the higher number of renovations in the early 1950s represents the homes being modernized with things like indoor plumbing or electricity but it's unclear. Due to this let's create new variables for whether or not the house was remodeled (a yes/no variable) and the relative age of the house (how long between the year it was sold and the year it was renovated). Homes that haven't been renovated or look dated can negatively impact the sale price.

```{r relative age vs price, echo = FALSE, message = FALSE, fig.pos='H', fig.cap= "Relative Age vs. Sale Price"}
train_raw <- train_raw %>% mutate(Remodeled = ifelse(YearBuilt == YearRemodAdd,
                                   0, 1))#0 = not remodeled, 1 = remodeled 
train_raw <- train_raw %>% mutate(RelAge = YrSold - YearRemodAdd) #how long since remodel
train_raw  %>%
  ggplot(aes(RelAge, SalePrice)) + geom_point() + geom_smooth() +
  ggtitle("Sale Price vs. Relative Age") + xlab("Relative Age (years)") +
  ylab("Sale Price (USD)")
#homes lose value the longer they go without a remodel
```

The rest of the parameters are more obscure and more difficult to determine the relationship with sale price. Before exploring some of these parameters we'll clean up some of the data by replacing any NAs with either a 0 for numeric data or None for categorical data. These operations are performed on both the train and test dataset.

```{r fill in the blanks, echo = FALSE, message = FALSE, warning = FALSE}
#inspecting these columns shows that most of the categorical data has NA = none 
#and numeric data NA = 0 with the exception of GarageYrBlt. Converting NAs to either
#"none" or "0" where appropriate.
train_raw <- train_raw %>% mutate_if(is.integer, ~replace(., is.na(.), 0)) %>%
  mutate_if(is.character, ~replace(., is.na(.), "None"))


#performing same operation on test set
test_raw <- test_raw %>% mutate_if(is.integer, ~replace(., is.na(.), 0)) %>%
  mutate_if(is.character, ~replace(., is.na(.), "None"))
```

There are `r length(names(which(sapply(train_raw, is.numeric))))` numeric variables and `r length(names(which(sapply(train_raw, is.character))))` categorical variables in the dataset. Comparing the lists of numeric and categorical variables shows that there is some overlap between the two. A major one is the "Quality" variables. Some use 1 through 10 and some use five abbreviations that are ordinal. The categorical variables this applies to are: Exterior Quality, Exterior Condition, Kitchen Quality, Fireplace Quality, Garage Quality, Garage Condition, Pool Quality, Heating Quality, Basement Quality, and Basement Condition. Fence Quality doesn't appear to have any sort of ranking for the quality abbreviations so that will be ignored. Below is an example of how Exterior Quality was modified. All others were modified in this fashion.

```{r numeric quality mods, message = FALSE}
#all quality params use Po = Poor, Fa = Fair, TA = Typicla/Average, Gd = Good, 
#Ex = Excellent
#I have added "None" above to replace NAs.
qual_vals <- c('None' = 0, 'Po' = 1, 'Fa' = 2, 'TA' = 3, 'Gd' = 4, 'Ex' = 5)

#Exterior Quality. Fixing train and test sets
train_raw$ExterQual <- as.integer(plyr::revalue(train_raw$ExterQual, qual_vals))
test_raw$ExterQual <- as.integer(plyr::revalue(test_raw$ExterQual, qual_vals))
```

The MSSubClasses variable, which identifies the type of dwelling, is numeric but not ordinal so that variable is converted to a factor.

```{r the rest of the quality varibles, echo = FALSE, message = FALSE, warning = FALSE }
#Exterior Condition has sames levels as quality. Fixing train and test sets
train_raw$ExterCond <- as.integer(plyr::revalue(train_raw$ExterCond, qual_vals))
test_raw$ExterCond <- as.integer(plyr::revalue(test_raw$ExterCond, qual_vals))

#Kitchen Quality. Fixing train and test sets
train_raw$KitchenQual <- as.integer(plyr::revalue(train_raw$KitchenQual, qual_vals))
test_raw$KitchenQual <- as.integer(plyr::revalue(test_raw$KitchenQual, qual_vals))

#Fireplace Quality. Fixing train and test sets
train_raw$FireplaceQu <- as.integer(plyr::revalue(train_raw$FireplaceQu, qual_vals))
test_raw$FireplaceQu <- as.integer(plyr::revalue(test_raw$FireplaceQu, qual_vals))

#Garage Quality. Fixing train and test sets
train_raw$GarageQual <- as.integer(plyr::revalue(train_raw$GarageQual, qual_vals))
test_raw$GarageQual <- as.integer(plyr::revalue(test_raw$GarageQual, qual_vals))

#Garage Condition appears to have the same levels as quality
train_raw$GarageCond <- as.integer(plyr::revalue(train_raw$GarageCond, qual_vals))
test_raw$GarageCond <- as.integer(plyr::revalue(test_raw$GarageCond, qual_vals))

#Pool Quality. Fixing train and test sets
train_raw$PoolQC <- as.integer(plyr::revalue(train_raw$PoolQC, qual_vals))
test_raw$PoolQC <- as.integer(plyr::revalue(test_raw$PoolQC, qual_vals))

#Heating Quality. Fixing train and test sets
train_raw$HeatingQC <- as.integer(plyr::revalue(train_raw$HeatingQC, qual_vals))
test_raw$HeatingQC  <- as.integer(plyr::revalue(test_raw$HeatingQC, qual_vals))

#Basement Quality actually evaluates height but uses the same rankings. Fixing train and test sets
train_raw$BsmtQual <- as.integer(plyr::revalue(train_raw$BsmtQual, qual_vals))
test_raw$BsmtQual  <- as.integer(plyr::revalue(test_raw$BsmtQual, qual_vals))

#Basement Condition actually is similar to garage condition. Fixing train and test sets
train_raw$BsmtCond <- as.integer(plyr::revalue(train_raw$BsmtCond, qual_vals))
test_raw$BsmtCond  <- as.integer(plyr::revalue(test_raw$BsmtCond, qual_vals))

#MSSubClass appears as numeric but after review it's clear this is categorical with
#numbers representing each category
train_raw$MSSubClass <- as.factor(train_raw$MSSubClass)
```

Separating the numeric variables from the categorical variables allows us to create a correlation matrix.  Since there are a large number of variables the matrix is filtered to show only the variables that have a greater than 0.5 correlation coefficient with sale price. 

```{r correlation matrix, echo = FALSE, message = FALSE, fig.pos='H', fig.cap= "Numeric Variable Correlation"}
#If we separate out the numeric features we can create a correlation matrix to see
#which features might be worth keeping/dropping.
train_numeric <- which(sapply(train_raw, is.numeric)) #find numeric vars
cor_mat_numeric <- cor(train_raw[,train_numeric]) #build correlation matrix 

#select only the stronger SalePrice correlation cases (e.g. abs(cor) > 0.5)
index_corr <- (which(abs(cor_mat_numeric[,'SalePrice']) > 0.5 , arr.ind = T))
#plot the reduced correlation matrix
corrplot::corrplot(cor_mat_numeric[index_corr,index_corr], 
                   method = "color",
                   addCoef.col = "black",
                   tl.col = "black", 
                   addgrid.col = "gray",
                   number.cex= 0.4,
                   number.digits = 2)
```

From this chart a list of variables to remove for prediction purposes can be created. We'll remove variables with poor correlation to sale price (less than or equal to 0.4). Also we'll remove "FullBath" due to high correlation with "TotalBaths". We'll remove "GrLivArea", "TotalBsmtSF", "1stFlrSF" due to high correlation with each other and "TotalSF". Then "YearBuilt" and "YearRemodAdd" are removed due to correlation with "RelAge". 

```{r start removing and factoring, echo = FALSE, message = FALSE, warning = FALSE}
#make list of params to ignore based on low correlation
corr_remove <- names(which(abs(cor_mat_numeric[,'SalePrice']) <= 0.4 , arr.ind = T))
corr_remove <- corr_remove[ corr_remove != "Id"] #need to keep house Id 
#add "FullBath" due to high cor w/ "TotalBaths"
#add "GrLivArea", "TotalBsmtSF", "1stFlrSF" due to high cor w/ "TotalSF"
#add "YearBuilt" and "YearRemodAdd" due to feature engineering
corr_remove <- append(corr_remove, c("GrLivArea",
                                     "TotalBsmtSF",
                                     "X1stFlrSF",
                                     "FullBath", 
                                     "YearBuilt",
                                     "YearRemodAdd"))


```

To look more closely at the categorical data the variables are factorized and then an Analysis of Variance (ANOVA) test is used to get an impression of how dependent sale price is on each variable. The ANOVA test (using the aov() function) provides the F statistic and p-values for each variable. Higher F statistics correspond to smaller p-values.  The smaller the p-value the more confident we can be that the variable does impact the sale price (confidence that we can reject the null hypothesis that it does not affect sale price).  The chart below shows the log transformation of the inverse p-value to show how much sale price depends on each variable. Clearly neighborhood dominates in value with MSZoning and lot shape also showing importance. Based on these results we'll remove any variables that have a log transformed inverse p-value that is less than 0.01 times the neighborhood value. Additionally, the near zero variance (nearZeroVar()) function is run on the entire train set.  The variables recommended are also removed. Finally, the outliers previously discussed are removed.

```{r removing unused variables, message = FALSE, fig.pos='H', fig.cap= "Categorical Variable Dependence"}
#use regression to look at categorical variables
#lets look at the variables that remain
char_names <- names(which(sapply(train_raw, is.character)))
str(train_raw[,char_names]) #doesn't appear to be any kind of ranking with the 
#remaining variables.
#convert remaining variables to factors.
train_raw[char_names] <- lapply(train_raw[char_names], factor)
#repeat for test data
char_names_test <- names(which(sapply(test_raw, is.character)))
test_raw[char_names_test] <- lapply(test_raw[char_names_test], factor)

#use ANOVA (aov()) to look at relationships between sale price and categorical variables
char_cor_mat <- train_raw[append(char_names,
                                 c("SalePrice","MSSubClass"))] #create categorical matrix
aov_cat_var <- broom::tidy(aov(SalePrice ~., 
                               char_cor_mat )) #run aov on the categorical matrix

aov_sum <- aov_cat_var %>% filter(!is.na(statistic)) %>%
  arrange(p.value) #summarize the aov test with ascending p-value

#plot by log(1/p.value) to show dependence
aov_sum  %>% mutate(term = reorder(term, p.value)) %>%
  ggplot(aes(term,log(1/p.value))) + geom_col() +
  ggtitle("Variable vs Log of inverse p-value") + xlab("Variable") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  ylab("Log of inverse p-value")
#looks like neighborhood, zoning, lot shape, bldg type, and basement exposure 
#might be valuable 

#pick variables to remove
remove_aov <- aov_sum %>% 
  filter(log(1/p.value) <= 0.01*max(log(1/p.value))) %>% pull(term)

#use nearzerovariance for pulling params out of model
nzv <- nearZeroVar(train_raw)
names(train_raw[,nzv]) #all these either had low F stat in anova or low cor
nzv_remove <- names(train_raw[,nzv]) #create list of cols to remove
remove_combined <-unique(c(corr_remove,
                           remove_aov,nzv_remove)) #combine all variables to be removed

#removing all previously discussed parameters
train_simplified <- train_raw[, !names(train_raw) %in% 
                                remove_combined] #remove all unused params

#removing square footage and bathroom outliers
train_simplified <- train_simplified[-append(outliers_sf,outliers_baths),]


```

```{r cleaning up variables, echo = FALSE, message = FALSE, warning = FALSE}
#clean as you go
rm(dl, char_names, char_names_test, colNAs, corr_remove, factor_vars, index_corr,
   numeric_vars, nzv, nzv_remove, qual_vals, remove_aov, train_numeric, outliers)
```

# Model Development Methods

The performance of the prediction model will be judged on the log RMSE defined here:

```{r log RMSE}
#Define log RMSE function on which the competition is judged. 
Log_RMSE <- function(actual_prices, pred_prices){
  sqrt(mean((log(actual_prices) - log(pred_prices))^2))
}
```

Since all training and testing of the models will have to occur on the provided train data set we need to partition the train dataset into train and test sets.  A partition of 90% for training and 10% for testing was selected since the train set is fairly small overall.

```{r create test and train sets, message = FALSE, warning = FALSE}
#split train set for train/test of algos
#first we need to divide the train set in to train/test sets.  Giving 10% to test
set.seed(1, sample.kind = "Rounding")
train_split_index <- createDataPartition(y = train_simplified$SalePrice, times = 1,
                                         p = 0.1, 
                                      list = FALSE)
split_train <- train_simplified[-train_split_index,]
split_test <- train_simplified[train_split_index,]
```

Due to the nature of the dataset having a mix a numeric and categorical data the types of algorithms that can be applied are limited to those that can handle both.  Random forests, gradient boosting, and regularized generalized linear models are all common algorithms that can be applied to this data and can all be found in the caret package documentation^1,2^.

## Random Forests

Random forests improves upon decision trees by averaging multiple decision trees. The decision trees are assembled randomly so they are unique.  To train the random forest model in caret I ran the Rborist method with a 10 fold cross validation. Rborist can be tuned on the minimum node size (minNode) and the number of trial predictors for a split (predFixed). For calculation time the number of trees is fixed to 50. The chart below shows the best tune created by the model.  The best tune is then fit with 1000 trees.
 

```{r random forest training, message = FALSE, warning = FALSE, fig.pos='H', fig.cap= "Random Forests Tuning"}
#starting with Random Forests (aka Rborist for tuning)
library(Rborist)
control <- trainControl(method="cv", number = 10, p = 0.8) #cross validation
grid <- expand.grid(minNode = c(1,5) , predFixed = seq(1,15,2)) #tuning params
train_rf <-  train(split_train[, !names(split_train) %in% c("Id","SalePrice")], 
                   log(split_train$SalePrice),
                   method = "Rborist",
                   nTree = 50,
                   trControl = control,
                   tuneGrid = grid,
                   nSamp = 5000)
ggplot(train_rf) + ggtitle("Random Forests Tuning Results") #plut tuning results
train_rf$bestTune #best  min node size and variables sampled at each split 
#now fit the rf model with the best tune params
fit_rf <- train(split_train[, !names(split_train) %in% c("Id","SalePrice")],
                log(split_train$SalePrice), 
                  method = "Rborist",
                  nTree = 1000,
                  tuneGrid = expand.grid(minNode = train_rf$bestTune$minNode,
                                         predFixed =train_rf$bestTune$predFixed))

pred_rf <- predict(fit_rf, 
                   split_test) #predict using best fit

rmse_rf <- Log_RMSE(split_test$SalePrice,
                    exp(pred_rf)) #reverse the log transformation for real

rmse_results <- tibble(Method = "Random Forests", LogRMSE = rmse_rf)

#make table
if(knitr::is_html_output()){
  knitr::kable(rmse_results, "html", caption = "RMSE Results") %>%
    kableExtra::kable_styling(bootstrap_options = "striped", full_width = FALSE)
} else{
  knitr::kable(rmse_results, "latex", booktabs = TRUE, caption = "RMSE Results") %>%
    kableExtra::kable_styling(font_size = 8, latex_options = "HOLD_position")
}
```

Making a prediction on the split test set shows a log RMSE of `r rmse_rf`. Let's try to improve.

## Gradient Boosting

Gradient boosting produces a prediction model in the form of an ensemble of weaker prediction models. A tree is fit to the dataset with all observations getting equal weight. Once the first tree is evaluated the weights of values are changed based on how easy or hard they are to predict and a new tree is fit. Subsequent trees are all built off of the previous which becomes an ensemble of weighted trees. Here the xgboost method is used. I've tuned based on the learning rate (eta) which adjusts the weights of features between boosting steps to make the process more/less conservative. I've also tuned by max_depth which is the maximum depth of a tree and I've tuned on min_child_weight which sets a minimum weight for the sum of instance in a leaf node.

The gradient boosting does not accept categorical variables.  Categorical variables must be converted to a numeric representation.  This is accomplished by one-hot encoding the categorical variables. Basically each category gets a column with 1 or 0.

```{r xgb training, message = FALSE, warning = FALSE, results= FALSE, fig.pos='H', fig.cap= "Gradient Boosting Tuning"}
#use dummyVars function to generate one-hot df of categorical variables.
split_train_num <- split_train[,names(which(sapply(split_train,
                                                   is.numeric)))] #create numeric df
split_train_cat <- split_train[,names(which(sapply(split_train,
                                                   is.factor)))] #create categorical df
dummy <- dummyVars("~.", data = split_train_cat) #create dummy data frame
dummy_df <- data.frame(predict(dummy,
                               newdata = split_train_cat)) #fill dummy df with categorical data

oh_split_train <- cbind(dummy_df,
                        split_train_num) #combine numeric and one-hot cat df into single df

xgb_control <- trainControl(method="cv", number = 5, p = 0.8) #cross validation
train_xgb <- train(x = oh_split_train[, !names(oh_split_train) %in% c("Id","SalePrice")],
                   y = log(oh_split_train$SalePrice),
                   method = "xgbTree", #initially tried xgbLinear but got high RMSEs
                   trControl = xgb_control,
                   tuneGrid = expand.grid(nrounds = 1000 , #tuning params
                                          eta = c(0.01, 0.05, 0.1),
                                          max_depth = c(2,3,4,5,6),
                                          gamma = 0,
                                          colsample_bytree = 1,
                                          min_child_weight = c(1,2,3,4,5),
                                          subsample = 1)
                   )
ggplot(train_xgb) + ggtitle("Gradient Boost Tuning Results") #plot tuning results

```

```{r xgb training p2, message = FALSE, warning = FALSE}
train_xgb$bestTune #best  min node size and variables sampled at each split 

```

```{r xgb training p3, message = FALSE, warning = FALSE, results = FALSE}
#now fit to best tune params
fit_xgb <- train(x = oh_split_train[, !names(oh_split_train) %in% c("Id","SalePrice")],
                 y = log(oh_split_train$SalePrice),
                 method = "xgbTree",
                 tuneGrid = expand.grid(nrounds = 1000 ,
                                        eta = train_xgb$bestTune$eta,
                                        max_depth = train_xgb$bestTune$max_depth,
                                        gamma = 0,
                                        colsample_bytree = 1,
                                        min_child_weight = train_xgb$bestTune$min_child_weight,
                                        subsample = 1))

#create one-hot encoded split_test for prediction
split_test_num <- split_test[,names(which(sapply(split_test, 
                                                 is.numeric)))] #create numeric df
split_test_cat <- split_test[,names(which(sapply(split_test, 
                                                 is.factor)))] #create categorical df
dummy <- dummyVars("~.", data = split_test_cat) #create dummy data frame
dummy_df <- data.frame(predict(dummy, 
                               newdata = split_test_cat)) #fill dummy df with categorical data

oh_split_test <- cbind(dummy_df,
                       split_test_num) #combine numeric and one-hot cat df into single df
pred_xgb <- predict(fit_xgb, 
                   oh_split_test)#make prediction on one-hot split test set


```

```{r xgb results, message = FALSE, warning = FALSE}
rmse_xgb <- Log_RMSE(oh_split_test$SalePrice,exp(pred_xgb)) #calc log RMSE
rmse_results <- bind_rows(rmse_results,
                          tibble(Method="Gradient Boosting",
                                 LogRMSE = rmse_xgb)) #add to the table
#make table
if(knitr::is_html_output()){
  knitr::kable(rmse_results, "html", caption = "RMSE Results") %>%
    kableExtra::kable_styling(bootstrap_options = "striped", full_width = FALSE)
} else{
  knitr::kable(rmse_results, "latex", booktabs = TRUE, caption = "RMSE Results") %>%
    kableExtra::kable_styling(font_size = 8, latex_options = "HOLD_position")
}
```


Here the log RMSE has improved over random forest to `r rmse_xgb`. 

## Regularized Generalized Linear Model

This is essentially a penalized glm that allows for control of mixing percentage between lasso and ridge regression (alpha between 0 and 1) as well as lambda (regularization parameter) that defines the amount of shrinkage. This is controlled via the tuneLength parameter (in my case try 6 alphas and 6 lambdas). The training set is also preprocessed to center (subtract the mean), scale (divide by standard deviation) and eliminate zero variance parameters.

```{r glm training, message = FALSE, warning = FALSE, fig.pos='H', fig.cap= "GLM Tuning"}
#glmnet implements a penalized glm model. Using one-hot encoded params
glm_control <- trainControl(method="cv", number = 10) #cross validation
train_glm <- train(x = oh_split_train[, !names(oh_split_train) %in% c("Id","SalePrice")],
                   y = log(oh_split_train$SalePrice),
                   method = "glmnet", 
                   preProcess = c("zv","center","scale"), #preprocess zv = zero variance
                   trControl = glm_control,
                   tuneLength = 6
)

ggplot(train_glm) + ggtitle("GLM Tuning Results") #plot tuning results
train_glm$bestTune #print best tune data
#now fit the best tune params 
fit_glm <- train(x = oh_split_train[, !names(oh_split_train) %in% c("Id","SalePrice")],
                   y = log(oh_split_train$SalePrice),
                   method = "glmnet", 
                   tuneGrid = expand.grid(
                     alpha = train_glm$bestTune$alpha, #mix between Lasso/Ridge regression
                                          lambda = train_glm$bestTune$lambda)
)
pred_glm <- predict(fit_glm, 
                    oh_split_test) #make prediction on the one-hot split test set
rmse_glm <- Log_RMSE(oh_split_test$SalePrice,exp(pred_glm)) #calculate the log RMSE

rmse_results <- bind_rows(rmse_results,
                          tibble(Method="GLM",
                                 LogRMSE = rmse_glm)) #add to the table
#make table
if(knitr::is_html_output()){
  knitr::kable(rmse_results, "html", caption = "RMSE Results") %>%
    kableExtra::kable_styling(bootstrap_options = "striped", full_width = FALSE)
} else{
  knitr::kable(rmse_results, "latex", booktabs = TRUE, caption = "RMSE Results") %>%
    kableExtra::kable_styling(font_size = 8, latex_options = "HOLD_position")
}
```

Here we see that the glm model has not necessarily improved on the gradient boosting model. Finally, let's create an ensemble of the average of all three models and compare. The ensemble prediction appears to slightly improve over the gradient boosted prediction.

```{r ensemble, echo = FALSE, message = FALSE, warning = FALSE}
#creating an ensemble of all methods (simple average)
pred_ens <- data.frame(Id = split_test$Id, SalePrice = (exp(pred_rf) + exp(pred_xgb) + exp(pred_glm))/3)

rmse_ens <- Log_RMSE(split_test$SalePrice,pred_ens$SalePrice) #calc new log rmse

rmse_results <- bind_rows(rmse_results,
                          tibble(Method="All Combined",
                                 LogRMSE = rmse_ens)) #add to the table
#make table
if(knitr::is_html_output()){
  knitr::kable(rmse_results, "html", caption = "RMSE Results") %>%
    kableExtra::kable_styling(bootstrap_options = "striped", full_width = FALSE)
} else{
  knitr::kable(rmse_results, "latex", booktabs = TRUE, caption = "RMSE Results") %>%
    kableExtra::kable_styling(font_size = 8, latex_options = "HOLD_position")
}
```

# Results

Now that we have tuned all three models and determined that the ensemble is the best predictor for sale price it's time to make final predictions on the validation set (the test.csv file).  Before predicting on those a final training of each model is performed on the entire (unsplit) training set using the tuning parameters determined previously.  The validation data is manipulated to match the training parameters. This includes removing NAs, reformatting the "Quality" parameters, factorizing parameters, removing unused parameters, and one-hot encoding the test data.  With one-hot encoding you end up creating a new column for each level of a factor parameter. In some cases there are levels that are only seen in one data set or the other (e.g. "None" for MSZoning is only seen in the validation set). These columns are removed since these algorithms can't predict on parameters they've never seen.

Once each algorithm is trained on the entire training set the final predictions are made on the validation set. A csv file is created for submission to Kaggle. The resulting predictions came out to a log RMSE of 0.13972.

```{r final predictions, echo = FALSE, message= FALSE, warning= FALSE, results = FALSE}
#########################################################
# PREDICTION ON HOLD OUT TEST SET FOR KAGGLE SUBMISSION
##########################################################
#using the original test set
#predict the sale price based on the test set params.  We'll need to perform some of cleaning
#that was performed on the training set. NAs and categorical data have already been addressed above.
#Need to add Total Sq Ft, Total Baths, Remodeled, and Relative Age Features
test_raw <- test_raw %>% mutate(TotalSF = GrLivArea + TotalBsmtSF) #add total sq ft
test_raw <- test_raw %>% mutate(TotalBaths = BsmtFullBath + 
                                    0.5*BsmtHalfBath + FullBath + 0.5*HalfBath) #add total baths
test_raw <- test_raw %>% mutate( Remodeled = ifelse(YearBuilt == YearRemodAdd,
                                                      0, 1))#0 = not remodeled, 1 = remodeled 
test_raw <- test_raw %>% mutate(RelAge = YrSold - YearRemodAdd) #how long since remodel
#now remove all the unused variables defined in the data cleaning of the train set
test_simplified <- test_raw[, !names(test_raw) %in% remove_combined] #remove all unused params

#one-hot encoding the test set 
test_simplified_num <- test_simplified[,names(which(sapply(test_simplified, is.numeric)))] #create numeric df
test_simplified_cat <- test_simplified[,names(which(sapply(test_simplified, is.factor)))] #create categorical df
dummy <- dummyVars("~.", data = test_simplified_cat) #create dummy data frame
dummy_df <- data.frame(predict(dummy, newdata = test_simplified_cat)) #fill dummy df with categorical data

oh_test_simplified <- cbind(dummy_df,test_simplified_num) #combine numeric and one-hot cat df into single df

#train each model on the combined unsplit train set before predicting on the test set
train_simplified_num <- train_simplified[,names(which(sapply(train_simplified, is.numeric)))] #create numeric df of full train set
train_simplified_cat <- train_simplified[,names(which(sapply(train_simplified, is.factor)))] #create categorical df full train set
dummy <- dummyVars("~.", data = train_simplified_cat) #create dummy data frame
dummy_df <- data.frame(predict(dummy, newdata = train_simplified_cat)) #fill dummy df with categorical data
oh_train_simplified <- cbind(dummy_df,train_simplified_num) #combine numeric and one-hot cat df into single df

#now look to see if there are params in test set not seen in training, then remove those params
unique_test_params <- names(oh_test_simplified)[!(names(oh_test_simplified) %in% 
                                                    names(oh_split_train))] #params uniqe to test
unique_train_params <- names(oh_train_simplified)[!(names(oh_train_simplified) %in% 
                                                      names(oh_test_simplified))] #params unique to train
unique_params <- c(unique_test_params,
                   unique_train_params[-which(unique_train_params == "SalePrice")]) #combine but remove saleprice
oh_test_simplified <- oh_test_simplified[, !names(oh_test_simplified) %in% 
                                           unique_params] #remove all unique params
oh_train_simplified <- oh_train_simplified[, !names(oh_train_simplified) %in% 
                                             unique_params] #remove all unique params

#run training on the entire test set using the optimized parameters.
final_fit_rf <- train(train_simplified[, !names(train_simplified) %in% c("Id","SalePrice")],
                log(train_simplified$SalePrice), 
                method = "Rborist",
                nTree = 1000,
                tuneGrid = expand.grid(minNode = train_rf$bestTune$minNode,
                                       predFixed =train_rf$bestTune$predFixed))

final_fit_xgb <- train(x = oh_train_simplified[, !names(oh_train_simplified) %in% c("Id","SalePrice")],
                 y = log(oh_train_simplified$SalePrice),
                 method = "xgbTree",
                 tuneGrid = expand.grid(nrounds = 1000 ,
                                        eta = train_xgb$bestTune$eta,
                                        max_depth = train_xgb$bestTune$max_depth,
                                        gamma = 0,
                                        colsample_bytree = 1,
                                        min_child_weight = train_xgb$bestTune$min_child_weight,
                                        subsample = 1))
final_fit_glm <- train(x = oh_train_simplified[, !names(oh_train_simplified) %in% c("Id","SalePrice")],
                 y = log(oh_train_simplified$SalePrice),
                 method = "glmnet", 
                 tuneGrid = expand.grid(alpha = train_glm$bestTune$alpha, 
                                        lambda = train_glm$bestTune$lambda)
)
#make final predictions 
pred_test_glm <- predict(final_fit_glm, 
                           oh_test_simplified)
pred_test_xgb <- predict(final_fit_xgb, 
                         oh_test_simplified)
pred_test_rf <- predict(final_fit_rf, 
                         test_simplified)
#create final ensemble
pred_test_ens <- data.frame(Id = test_simplified$Id, SalePrice = (exp(pred_test_glm) + exp(pred_test_xgb) + 
                                                                    exp(pred_test_rf))/3)
#write csv file to submit to kaggle
write.csv(pred_test_ens, file = "Ames_Price_predictions.csv", row.names = FALSE)
```

# Conclusion

Data analysis and visualization showed the relationship between many of the parameters and sale price. Three algorithms were combined in an ensemble to predict the sale price of 1459 homes.  The log residual mean squared error (RMSE) was used to validate the model with a final log RMSE of 0.13972. This model could be potentially be improved further by expanding the data analysis and cleaning. It's possible there are other features that could be eliminated or combined. There is a large library of algorithms that could be applied to this data so it's possible there are better ones out there as well or that a better, larger ensemble could be created.  


## References

1) [topepo.github.io/caret/available-models.html](https://topepo.github.io/caret/available-models.html)
2) [topepo.github.io/caret/train-models-by-tag.html](https://topepo.github.io/caret/train-models-by-tag.html)